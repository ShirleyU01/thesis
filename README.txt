# How to run the files to achieve automate streamline

##########################################################################################################
## Related Files:
# read.py -- read all generated implementations, insert them into placeholder, and save in llms/implementation folder
# run.py -- run all implementations through testcases and save results in a csv file called `info.csv`
# refinement_prompt.py -- generate refinement prompts for failed implementations (fail on compile/testcases)
# bleu_check.py -- check the diversity of implementations 

# NO NEED TO USE - process.py -- find and replace wrong module/function name, no need for fixed prompt
# NO NEED TO USE - reformat.py -- clear up inserted implementation code, no need to do
# NO NEED TO USE - parse_script.py -- functions to extract information of failled testcases, used in run.py

## Related Output Folders:
# /llms/compile
# /llms/implementation
# /diversity

##########################################################################################################
### Streamline
## Step 1 - Process GPT-Generated Implementation from JSON
# open read.py, change line 10, 12 to target folder path 
# under thesis directory, run `python read.py`
# check directory llms/implementation/TARGET_PATH, all implementations should be saved there

## Step 2 - Run Implementations
# open run.py, change line 57, 58, 59 to target folder path
# under thesis directory, run `python run.py`
# check directory llms/compile/TARGET_PATH, there should be a file called `info.csv` that stores the following information:
Col 1. Implementation - filename
Col 2. Complie - if not compile, compile error message will be saved; otherwise will be empty
Col 3. Error - if not compile/fail on any testcases, compile error message/failed test info will be saved; otherwise will be empty

## Step 3 - Refinement Based on `info.csv` Generated by Step 2
# open refinement_prompt.py, change line 18 to target folder path
# uncomment line 78, 80, under thesis directory, run `python refinement_prompt.py` to generate all refinement prompts

## Step 4 - Calculate BLEU Score for Implementations
# open bleu_check.py, change line 6 to target folder path
# under thesis directory, run `python bleu_check.py`
# check directory thesis/diversity/TARGET_PATH, there should be a csv file called `analysis.csv`, which stores score range/avg, similar implementation pair





